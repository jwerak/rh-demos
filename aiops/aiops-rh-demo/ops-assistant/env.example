# LiteLLM Configuration
# LiteLLM supports 100+ LLM providers. Set the appropriate API key(s) for your provider:
# For OpenAI: OPENAI_API_KEY
# For Anthropic: ANTHROPIC_API_KEY
# For Azure: AZURE_API_KEY, AZURE_API_BASE, AZURE_API_VERSION
# For AWS Bedrock: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME
# For custom/local OpenAI-compatible endpoints: OPENAI_API_KEY, OPENAI_API_BASE
OPENAI_API_KEY=your-api-key-here
OPENAI_API_BASE=https://your-openai-compatible-endpoint.com/v1

# MCP Server Configuration
MCP_SERVER_URL=https://mcp-server-aap.apps.cluster.example.com/mcp
# Set to 'false' to disable SSL verification (useful for self-signed certificates)
# WARNING: Only disable for development/testing. Never in production!
MCP_VERIFY_SSL=true

# Model Configuration
# LiteLLM model format examples:
# - OpenAI: "gpt-4", "gpt-3.5-turbo"
# - Anthropic: "claude-3-opus-20240229", "claude-3-sonnet-20240229"
# - Azure: "azure/gpt-4-deployment-name"
# - AWS Bedrock: "bedrock/anthropic.claude-v2"
# - Custom OpenAI-compatible: "openai/DeepSeek-R1-Distill-Qwen-14B-W4A16"
MODEL_NAME=openai/DeepSeek-R1-Distill-Qwen-14B-W4A16

# Webhook Configuration
WEBHOOK_PATH=764de33f-7e2f-447b-b55c-f769361480fb

# Server Configuration
SERVER_HOST=0.0.0.0
SERVER_PORT=5678

# Logging
LOG_LEVEL=INFO

