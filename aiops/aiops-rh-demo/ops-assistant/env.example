# LiteLLM Configuration
# LiteLLM supports 100+ LLM providers. Set the appropriate API key(s) for your provider:
# For OpenAI: OPENAI_API_KEY
# For Anthropic: ANTHROPIC_API_KEY
# For Google Gemini: GEMINI_API_KEY (get from https://aistudio.google.com/app/apikey)
# For Azure: AZURE_API_KEY, AZURE_API_BASE, AZURE_API_VERSION
# For AWS Bedrock: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME
# For custom/local OpenAI-compatible endpoints: OPENAI_API_KEY, OPENAI_API_BASE

# === OPTION 1: OpenAI ===
OPENAI_API_KEY=your-api-key-here
# OPENAI_API_BASE=https://api.openai.com/v1  # Optional, uses this by default

# === OPTION 2: Google Gemini (RECOMMENDED - Free tier available, good function calling) ===
# GEMINI_API_KEY=your-gemini-api-key-here

# === OPTION 3: Anthropic Claude ===
# ANTHROPIC_API_KEY=your-anthropic-api-key-here

# === OPTION 4: Custom/Local OpenAI-compatible endpoint ===
# OPENAI_API_BASE=https://your-openai-compatible-endpoint.com/v1
# OPENAI_API_KEY=dummy-key-for-local

# MCP Server Configuration
MCP_SERVER_URL=https://mcp-server-aap.apps.cluster.example.com/mcp
# Set to 'false' to disable SSL verification (useful for self-signed certificates)
# WARNING: Only disable for development/testing. Never in production!
MCP_VERIFY_SSL=true

# Model Configuration
# LiteLLM model format examples with function calling support:
#
# === RECOMMENDED: Best for function calling ===
# - Google Gemini (FREE tier, excellent function calling):
#   MODEL_NAME=gemini/gemini-1.5-flash-latest      # Fast, free tier, good quality
#   MODEL_NAME=gemini/gemini-1.5-pro-latest        # Best quality, free tier
#   MODEL_NAME=gemini/gemini-2.0-flash-exp         # Experimental, very fast
#
# - OpenAI (Paid, most reliable):
#   MODEL_NAME=gpt-4o                              # Best overall
#   MODEL_NAME=gpt-4-turbo                         # Very good
#   MODEL_NAME=gpt-3.5-turbo                       # Cheap, reliable
#
# - Anthropic Claude (Paid, excellent):
#   MODEL_NAME=claude-3-5-sonnet-20241022          # Excellent reasoning
#   MODEL_NAME=claude-3-5-haiku-20241022           # Fast, cheaper
#
# === OTHER OPTIONS ===
# - Azure: "azure/gpt-4-deployment-name"
# - AWS Bedrock: "bedrock/anthropic.claude-v2"
# - Custom OpenAI-compatible: "openai/model-name"
#
MODEL_NAME=gemini/gemini-1.5-flash-latest

# Temperature controls randomness in model responses (0.0 to 2.0)
# - 0.0: Deterministic, focused (best for factual tasks, function calling)
# - 0.7: Balanced creativity and focus
# - 1.0: More creative and varied responses
# - 2.0: Maximum creativity (may be less coherent)
MODEL_TEMPERATURE=0

# Webhook Configuration
WEBHOOK_PATH=764de33f-7e2f-447b-b55c-f769361480fb

# Server Configuration
SERVER_HOST=0.0.0.0
SERVER_PORT=5678

# Logging
LOG_LEVEL=INFO

# Color Output (for container environments)
# Set to '1' to force colored output in containers/pipes
FORCE_COLOR=1

# Agent Configuration
# Maximum number of reasoning steps before stopping (default: 50)
# Increase if you see "Recursion limit reached" errors
RECURSION_LIMIT=50

# Tool Retry Configuration
# Enable/disable automatic retry wrapper for MCP tools (default: true)
# Set to 'false' if tools are failing with "'NoneType' object is not callable"
ENABLE_TOOL_RETRY=true

# Maximum number of retry attempts for failed MCP tool calls (default: 3)
# Only applies if ENABLE_TOOL_RETRY=true
MAX_TOOL_RETRIES=3

# Tool Choice Configuration
# Controls whether to pass tool_choice parameter to the LLM
# Options:
#   "auto" - Let model decide when to use tools (default, works for cloud models)
#   "required" - Force model to always use a tool (more aggressive)
#   "none" - Don't prefer tools, let model decide freely
#   "false" - Don't send tool_choice at all (best for custom endpoints)
# If your model generates TEXT about tools instead of calling them, try "false"
ENABLE_TOOL_CHOICE=auto

